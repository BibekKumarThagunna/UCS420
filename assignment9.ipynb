{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c657df-f6a4-4cdc-844f-697078ab3480",
   "metadata": {},
   "source": [
    "# Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuation.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Remove stopwords (using NLTK's stopwords list).\n",
    "4. Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b782ef5-dc58-46b2-941d-fdbe67508a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d83fd19-7820-42a1-bc68-0c62b275f853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in /home/sybar/miniconda3/lib/python3.13/site-packages (from nltk) (1.5.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /home/sybar/miniconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b355d0fe-3cd5-4392-89ba-c2ab6cfe471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sybar/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/sybar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/sybar/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /home/sybar/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6048a720-d227-491e-b378-f2492f6bd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Technology, a relentless tide of innovation, continues to reshape the human experience at an astonishing pace. It weaves itself into the fabric of our daily lives, from the algorithms that curate our information to the unseen networks powering global communication. This constant evolution presents a fascinating paradox: offering unprecedented connectivity and convenience while simultaneously raising complex questions about privacy, equity, and the very nature of work and interaction in an increasingly digitized world. Navigating this ever-changing landscape requires not just adoption, but also critical reflection on how we harness technology for collective progress and well-being.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f136109-6c41-403e-8400-59c8a5a083a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "technology a relentless tide of innovation continues to reshape the human experience at an astonishing pace it weaves itself into the fabric of our daily lives from the algorithms that curate our information to the unseen networks powering global communication this constant evolution presents a fascinating paradox offering unprecedented connectivity and convenience while simultaneously raising complex questions about privacy equity and the very nature of work and interaction in an increasingly digitized world navigating this everchanging landscape requires not just adoption but also critical reflection on how we harness technology for collective progress and wellbeing\n"
     ]
    }
   ],
   "source": [
    "text_lower = text.lower()\n",
    "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b8a947-fd38-4b71-9698-2a0c5db54743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technology', 'a', 'relentless', 'tide', 'of', 'innovation', 'continues', 'to', 'reshape', 'the', 'human', 'experience', 'at', 'an', 'astonishing', 'pace', 'it', 'weaves', 'itself', 'into', 'the', 'fabric', 'of', 'our', 'daily', 'lives', 'from', 'the', 'algorithms', 'that', 'curate', 'our', 'information', 'to', 'the', 'unseen', 'networks', 'powering', 'global', 'communication', 'this', 'constant', 'evolution', 'presents', 'a', 'fascinating', 'paradox', 'offering', 'unprecedented', 'connectivity', 'and', 'convenience', 'while', 'simultaneously', 'raising', 'complex', 'questions', 'about', 'privacy', 'equity', 'and', 'the', 'very', 'nature', 'of', 'work', 'and', 'interaction', 'in', 'an', 'increasingly', 'digitized', 'world', 'navigating', 'this', 'everchanging', 'landscape', 'requires', 'not', 'just', 'adoption', 'but', 'also', 'critical', 'reflection', 'on', 'how', 'we', 'harness', 'technology', 'for', 'collective', 'progress', 'and', 'wellbeing']\n",
      "['\\ntechnology a relentless tide of innovation continues to reshape the human experience at an astonishing pace it weaves itself into the fabric of our daily lives from the algorithms that curate our information to the unseen networks powering global communication this constant evolution presents a fascinating paradox offering unprecedented connectivity and convenience while simultaneously raising complex questions about privacy equity and the very nature of work and interaction in an increasingly digitized world navigating this everchanging landscape requires not just adoption but also critical reflection on how we harness technology for collective progress and wellbeing']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text_clean)\n",
    "sent_tokens = sent_tokenize(text_clean)\n",
    "print(word_tokens)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f1d41e9-343a-4808-8df7-67ceeee8eb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technology', 'relentless', 'tide', 'innovation', 'continues', 'reshape', 'human', 'experience', 'astonishing', 'pace', 'weaves', 'fabric', 'daily', 'lives', 'algorithms', 'curate', 'information', 'unseen', 'networks', 'powering', 'global', 'communication', 'constant', 'evolution', 'presents', 'fascinating', 'paradox', 'offering', 'unprecedented', 'connectivity', 'convenience', 'simultaneously', 'raising', 'complex', 'questions', 'privacy', 'equity', 'nature', 'work', 'interaction', 'increasingly', 'digitized', 'world', 'navigating', 'everchanging', 'landscape', 'requires', 'adoption', 'also', 'critical', 'reflection', 'harness', 'technology', 'collective', 'progress', 'wellbeing']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d658086-4e52-4fad-8d48-7ee391147e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequency Distribution (excluding stopwords):\n",
      "[('technology', 2), ('relentless', 1), ('tide', 1), ('innovation', 1), ('continues', 1), ('reshape', 1), ('human', 1), ('experience', 1), ('astonishing', 1), ('pace', 1), ('weaves', 1), ('fabric', 1), ('daily', 1), ('lives', 1), ('algorithms', 1), ('curate', 1), ('information', 1), ('unseen', 1), ('networks', 1), ('powering', 1), ('global', 1), ('communication', 1), ('constant', 1), ('evolution', 1), ('presents', 1), ('fascinating', 1), ('paradox', 1), ('offering', 1), ('unprecedented', 1), ('connectivity', 1), ('convenience', 1), ('simultaneously', 1), ('raising', 1), ('complex', 1), ('questions', 1), ('privacy', 1), ('equity', 1), ('nature', 1), ('work', 1), ('interaction', 1), ('increasingly', 1), ('digitized', 1), ('world', 1), ('navigating', 1), ('everchanging', 1), ('landscape', 1), ('requires', 1), ('adoption', 1), ('also', 1), ('critical', 1), ('reflection', 1), ('harness', 1), ('collective', 1), ('progress', 1), ('wellbeing', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "freq_dist = nltk.FreqDist(filtered_words)\n",
    "\n",
    "print(\"Word Frequency Distribution (excluding stopwords):\")\n",
    "print(freq_dist.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f1410-7712-44a8-a8b5-b8ad4411053b",
   "metadata": {},
   "source": [
    "# Q2: Stemming and Lemmatization\n",
    "1. Take the tokenized words from QuesƟon 1 (after stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmatization using NLTK's WordNetLemmatizer.\n",
    "4. Compare and display results of both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab6bac9f-89d9-489f-9d47-8efcb2665fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d1af171-eec1-4220-bc7f-a33ca9b7ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmed = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in filtered_words]\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "258a91e1-091d-4cd4-91c2-f1ddc6a56959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porter Stemmer: ['technolog', 'relentless', 'tide', 'innov', 'continu', 'reshap', 'human', 'experi', 'astonish', 'pace', 'weav', 'fabric', 'daili', 'live', 'algorithm', 'curat', 'inform', 'unseen', 'network', 'power', 'global', 'commun', 'constant', 'evolut', 'present', 'fascin', 'paradox', 'offer', 'unpreced', 'connect', 'conveni', 'simultan', 'rais', 'complex', 'question', 'privaci', 'equiti', 'natur', 'work', 'interact', 'increasingli', 'digit', 'world', 'navig', 'everchang', 'landscap', 'requir', 'adopt', 'also', 'critic', 'reflect', 'har', 'technolog', 'collect', 'progress', 'wellb']\n",
      "\n",
      "Lancaster Stemmer: ['technolog', 'relentless', 'tid', 'innov', 'continu', 'reshap', 'hum', 'expery', 'aston', 'pac', 'weav', 'fabr', 'dai', 'liv', 'algorithm', 'cur', 'inform', 'unseen', 'network', 'pow', 'glob', 'commun', 'const', 'evolv', 'pres', 'fascin', 'paradox', 'off', 'unprec', 'connect', 'conveny', 'simult', 'rais', 'complex', 'quest', 'priv', 'equ', 'nat', 'work', 'interact', 'increas', 'digit', 'world', 'navig', 'everchang', 'landscap', 'requir', 'adopt', 'also', 'crit', 'reflect', 'har', 'technolog', 'collect', 'progress', 'wellb']\n",
      "\n",
      "Lemmatized: ['technology', 'relentless', 'tide', 'innovation', 'continues', 'reshape', 'human', 'experience', 'astonishing', 'pace', 'weave', 'fabric', 'daily', 'life', 'algorithm', 'curate', 'information', 'unseen', 'network', 'powering', 'global', 'communication', 'constant', 'evolution', 'present', 'fascinating', 'paradox', 'offering', 'unprecedented', 'connectivity', 'convenience', 'simultaneously', 'raising', 'complex', 'question', 'privacy', 'equity', 'nature', 'work', 'interaction', 'increasingly', 'digitized', 'world', 'navigating', 'everchanging', 'landscape', 'requires', 'adoption', 'also', 'critical', 'reflection', 'harness', 'technology', 'collective', 'progress', 'wellbeing']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPorter Stemmer:\", porter_stemmed)\n",
    "print(\"\\nLancaster Stemmer:\", lancaster_stemmed)\n",
    "print(\"\\nLemmatized:\", lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d646b0-09b7-4256-af4b-7c2350307d5f",
   "metadata": {},
   "source": [
    "# Q3. Regular Expressions and Text Splitting\n",
    "\n",
    "- 1. Take the original text from Question 1.\n",
    "- 2. Use regular expressions to:\n",
    "  - a. Extract all words with more than 5 letters.\n",
    "  - b. Extract all numbers (if any exist in their text).\n",
    "  - c. Extract all capitalized words.\n",
    "- 3. Use text splitting techniques to:\n",
    "  - a. Split the text into words containing only alphabets (removing digits and special characters).\n",
    "  - b. Extract words starting with a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf6a3e3-8931-44fe-b273-469f55bf54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_gt_5 = re.findall(r'\\b\\w{6,}\\b', text)\n",
    "numbers = re.findall(r'\\d+', text)\n",
    "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "vowel_words = [word for word in alpha_words if word[0].lower() in 'aeiou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "297af476-ce40-4b12-8ffa-cb11be239378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words > 5 letters: ['Technology', 'relentless', 'innovation', 'continues', 'reshape', 'experience', 'astonishing', 'weaves', 'itself', 'fabric', 'algorithms', 'curate', 'information', 'unseen', 'networks', 'powering', 'global', 'communication', 'constant', 'evolution', 'presents', 'fascinating', 'paradox', 'offering', 'unprecedented', 'connectivity', 'convenience', 'simultaneously', 'raising', 'complex', 'questions', 'privacy', 'equity', 'nature', 'interaction', 'increasingly', 'digitized', 'Navigating', 'changing', 'landscape', 'requires', 'adoption', 'critical', 'reflection', 'harness', 'technology', 'collective', 'progress']\n",
      "\n",
      "Numbers: []\n",
      "\n",
      "Capitalized Words: ['Technology', 'It', 'This', 'Navigating']\n",
      "\n",
      "Alphabetic Words: ['Technology', 'a', 'relentless', 'tide', 'of', 'innovation', 'continues', 'to', 'reshape', 'the', 'human', 'experience', 'at', 'an', 'astonishing', 'pace', 'It', 'weaves', 'itself', 'into', 'the', 'fabric', 'of', 'our', 'daily', 'lives', 'from', 'the', 'algorithms', 'that', 'curate', 'our', 'information', 'to', 'the', 'unseen', 'networks', 'powering', 'global', 'communication', 'This', 'constant', 'evolution', 'presents', 'a', 'fascinating', 'paradox', 'offering', 'unprecedented', 'connectivity', 'and', 'convenience', 'while', 'simultaneously', 'raising', 'complex', 'questions', 'about', 'privacy', 'equity', 'and', 'the', 'very', 'nature', 'of', 'work', 'and', 'interaction', 'in', 'an', 'increasingly', 'digitized', 'world', 'Navigating', 'this', 'ever', 'changing', 'landscape', 'requires', 'not', 'just', 'adoption', 'but', 'also', 'critical', 'reflection', 'on', 'how', 'we', 'harness', 'technology', 'for', 'collective', 'progress', 'and', 'well', 'being']\n",
      "\n",
      "Words starting with vowels: ['a', 'of', 'innovation', 'experience', 'at', 'an', 'astonishing', 'It', 'itself', 'into', 'of', 'our', 'algorithms', 'our', 'information', 'unseen', 'evolution', 'a', 'offering', 'unprecedented', 'and', 'about', 'equity', 'and', 'of', 'and', 'interaction', 'in', 'an', 'increasingly', 'ever', 'adoption', 'also', 'on', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWords > 5 letters:\", words_gt_5)\n",
    "print(\"\\nNumbers:\", numbers)\n",
    "print(\"\\nCapitalized Words:\", capitalized)\n",
    "print(\"\\nAlphabetic Words:\", alpha_words)\n",
    "print(\"\\nWords starting with vowels:\", vowel_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c97fe-6988-4915-b565-48ba7d6ee1e5",
   "metadata": {},
   "source": [
    "# Q4. Custom Tokenization & Regex-based Text Cleaning\n",
    "\n",
    "- 1. Take original text from Question 1.\n",
    "- 2. Write a custom tokenization function that:\n",
    "   - a. Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").\n",
    "   - b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).\n",
    "   - c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
    "- 3. Use Regex Substitutions (re.sub) to:\n",
    "   - a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "   - b. Replace URLs with '<URL>' placeholder.\n",
    "   - c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with '<PHONE>' placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ddc211a-b3ca-4181-9dc0-0dc439784096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Tokens: ['Technology', 'a', 'relentless', 'tide', 'of', 'innovation', 'continues', 'to', 'reshape', 'the', 'human', 'experience', 'at', 'an', 'astonishing', 'pace', 'It', 'weaves', 'itself', 'into', 'the', 'fabric', 'of', 'our', 'daily', 'lives', 'from', 'the', 'algorithms', 'that', 'curate', 'our', 'information', 'to', 'the', 'unseen', 'networks', 'powering', 'global', 'communication', 'This', 'constant', 'evolution', 'presents', 'a', 'fascinating', 'paradox', 'offering', 'unprecedented', 'connectivity', 'and', 'convenience', 'while', 'simultaneously', 'raising', 'complex', 'questions', 'about', 'privacy', 'equity', 'and', 'the', 'very', 'nature', 'of', 'work', 'and', 'interaction', 'in', 'an', 'increasingly', 'digitized', 'world', 'Navigating', 'this', 'ever-changing', 'landscape', 'requires', 'not', 'just', 'adoption', 'but', 'also', 'critical', 'reflection', 'on', 'how', 'we', 'harness', 'technology', 'for', 'collective', 'progress', 'and', 'well-being']\n",
      "\n",
      "Text after substitutions: \n",
      "Technology, a relentless tide of innovation, continues to reshape the human experience at an astonishing pace. It weaves itself into the fabric of our daily lives, from the algorithms that curate our information to the unseen networks powering global communication. This constant evolution presents a fascinating paradox: offering unprecedented connectivity and convenience while simultaneously raising complex questions about privacy, equity, and the very nature of work and interaction in an increasingly digitized world. Navigating this ever-changing landscape requires not just adoption, but also critical reflection on how we harness technology for collective progress and well-being.\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    pattern = r\"\\b(?:[a-zA-Z]+(?:'[a-z]+)?(?:-[a-z]+)*|\\d+(?:\\.\\d+)?)\\b\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "custom_tokens = custom_tokenizer(text)\n",
    "\n",
    "text_sub = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "text_sub = re.sub(r'http\\S+|www\\.\\S+', '<URL>', text_sub)\n",
    "text_sub = re.sub(r'\\b(?:\\+91[-\\s]?|0)?\\d{10}\\b|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', text_sub)\n",
    "\n",
    "print(\"\\nCustom Tokens:\", custom_tokens)\n",
    "print(\"\\nText after substitutions:\", text_sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
